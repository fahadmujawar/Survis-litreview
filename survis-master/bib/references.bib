@article{YI2019101552,
title = {Generative adversarial network in medical imaging: A review},
journal = {Medical Image Analysis},
volume = {58},
pages = {101552},
year = {2019},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2019.101552},
url = {https://www.sciencedirect.com/science/article/pii/S1361841518308430},
author = {Xin Yi and Ekta Walia and Paul Babyn},
keywords = {Deep learning, Generative adversarial network, Generative model, Medical imaging, Review},
abstract = {Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.}
}
@INPROCEEDINGS{8363564,
  author={Chuquicusma, Maria J. M. and Hussein, Sarfaraz and Burt, Jeremy and Bagci, Ulas},
  booktitle={2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)}, 
  title={How to fool radiologists with generative adversarial networks? A visual turing test for lung cancer diagnosis}, 
  year={2018},
  volume={},
  number={},
  pages={240-244},
  keywords={Cancer;Lung;Visualization;Computed tomography;Generators;Gallium nitride;Lung nodules;Generated samples;Visual Turing Test;Computed Tomography (CT);Deep learning;Generative Adversarial Networks (GANs);Computer Aided Diagnosis (CAD) systems},
  doi={10.1109/ISBI.2018.8363564}}

@INPROCEEDINGS{8363576,
  author={Frid-Adar, Maayan and Klang, Eyal and Amitai, Michal and Goldberger, Jacob and Greenspan, Hayit},
  booktitle={2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)}, 
  title={Synthetic data augmentation using GAN for improved liver lesion classification}, 
  year={2018},
  volume={},
  number={},
  pages={289-293},
  keywords={Lesions;Liver;Gallium nitride;Training;Medical diagnostic imaging;Task analysis;Image synthesis;data augmentation;generative adversarial network;liver lesions;lesion classification},
  doi={10.1109/ISBI.2018.8363576}}

@article{KAZEMINIA2020101938,
title = {GANs for medical image analysis},
journal = {Artificial Intelligence in Medicine},
volume = {109},
pages = {101938},
year = {2020},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2020.101938},
url = {https://www.sciencedirect.com/science/article/pii/S0933365719311510},
author = {Salome Kazeminia and Christoph Baur and Arjan Kuijper and Bram {van Ginneken} and Nassir Navab and Shadi Albarqouni and Anirban Mukhopadhyay},
keywords = {Generative adversarial networks, Deep learning, Medical imaging, Survey},
abstract = {Generative adversarial networks (GANs) and their extensions have carved open many exciting ways to tackle well known and challenging medical image analysis problems such as medical image de-noising, reconstruction, segmentation, data simulation, detection or classification. Furthermore, their ability to synthesize images at unprecedented levels of realism also gives hope that the chronic scarcity of labeled data in the medical field can be resolved with the help of these generative models. In this review paper, a broad overview of recent literature on GANs for medical applications is given, the shortcomings and opportunities of the proposed methods are thoroughly discussed, and potential future work is elaborated. We review the most relevant papers published until the submission date. For quick access, essential details such as the underlying method, datasets, and performance are tabulated. An interactive visualization that categorizes all papers to keep the review alive is available at http://livingreview.in.tum.de/GANs_for_Medical_Applications/.}
}

@article{UsmanAkbar2024,
  author       = {Muhammad Usman Akbar and Måns Larsson and Ida Blystad and Anders Eklund},
  title        = {Brain tumor segmentation using synthetic MR images - A comparison of GANs and diffusion models},
  journal      = {Scientific Data},
  year         = {2024},
  volume       = {11},
  number       = {1},
  pages        = {259},
  doi          = {10.1038/s41597-024-03073-x},
  url          = {https://doi.org/10.1038/s41597-024-03073-x},
  abstract     = {Large annotated datasets are required for training deep learning models, but in medical imaging data sharing is often complicated due to ethics, anonymization and data protection legislation. Generative AI models, such as generative adversarial networks (GANs) and diffusion models, can today produce very realistic synthetic images, and can potentially facilitate data sharing. However, in order to share synthetic medical images it must first be demonstrated that they can be used for training different networks with acceptable performance. Here, we therefore comprehensively evaluate four GANs (progressive GAN, StyleGAN 1–3) and a diffusion model for the task of brain tumor segmentation (using two segmentation networks, U-Net and a Swin transformer). Our results show that segmentation networks trained on synthetic images reach Dice scores that are 80%–90% of Dice scores when training with real images, but that memorization of the training images can be a problem for diffusion models if the original dataset is too small. Our conclusion is that sharing synthetic medical images is a viable option to sharing real images, but that further work is required. The trained generative models and the generated synthetic images are shared on AIDA data hub.},
  issn         = {2052-4463}
}


@Article{bioengineering10111258,
AUTHOR = {Hung, Alex Ling Yu and Zhao, Kai and Zheng, Haoxin and Yan, Ran and Raman, Steven S. and Terzopoulos, Demetri and Sung, Kyunghyun},
TITLE = {Med-cDiff: Conditional Medical Image Generation with Diffusion Models},
JOURNAL = {Bioengineering},
VOLUME = {10},
YEAR = {2023},
NUMBER = {11},
ARTICLE-NUMBER = {1258},
URL = {https://www.mdpi.com/2306-5354/10/11/1258},
PubMedID = {38002382},
ISSN = {2306-5354},
ABSTRACT = {Conditional image generation plays a vital role in medical image analysis as it is effective in tasks such as super-resolution, denoising, and inpainting, among others. Diffusion models have been shown to perform at a state-of-the-art level in natural image generation, but they have not been thoroughly studied in medical image generation with specific conditions. Moreover, current medical image generation models have their own problems, limiting their usage in various medical image generation tasks. In this paper, we introduce the use of conditional Denoising Diffusion Probabilistic Models (cDDPMs) for medical image generation, which achieve state-of-the-art performance on several medical image generation tasks.},
DOI = {10.3390/bioengineering10111258}
}





@article{ refId0,
	author = {{Lin, Run} and {Li, Chunxiao} and {Wang, Ruixuan}},
	title = {A Benchmark for Multi-Task Evaluation of Pretrained Models in Medical Report Generation},
	DOI= "10.1051/bioconf/202517403010",
	url= "https://doi.org/10.1051/bioconf/202517403010",
	journal = {BIO Web Conf.},
	year = 2025,
	volume = 174,
	pages = "03010",
}


@article{SORTINO2023103721,
title = {Transformer-based image generation from scene graphs},
journal = {Computer Vision and Image Understanding},
volume = {233},
pages = {103721},
year = {2023},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2023.103721},
url = {https://www.sciencedirect.com/science/article/pii/S1077314223001017},
author = {Renato Sortino and Simone Palazzo and Francesco Rundo and Concetto Spampinato},
keywords = {Scene graphs, Transformers, Generative models, Conditional image generation},
abstract = {Graph-structured scene descriptions can be efficiently used in generative models to control the composition of the generated image. Previous approaches are based on the combination of graph convolutional networks and adversarial methods for layout prediction and image generation, respectively. In this work, we show how employing multi-head attention to encode the graph information, as well as using a transformer-based model in the latent space for image generation can improve the quality of the sampled data, without the need to employ adversarial models with the subsequent advantage in terms of training stability. The proposed approach, specifically, is entirely based on transformer architectures both for encoding scene graphs into intermediate object layouts and for decoding these layouts into images, passing through a lower dimensional space learned by a vector-quantized variational autoencoder. Our approach shows an improved image quality with respect to state-of-the-art methods as well as a higher degree of diversity among multiple generations from the same scene graph. We evaluate our approach on three public datasets: Visual Genome, COCO, and CLEVR. We achieve an Inception Score of 13.7 and 12.8, and an FID of 52.3 and 60.3, on COCO and Visual Genome, respectively. We perform ablation studies on our contributions to assess the impact of each component. Code is available at https://github.com/perceivelab/trf-sg2im.}
}

@article{Izhar2025,
  author       = {Amaan Izhar and Norisma Idris and Nurul Japar},
  title        = {Engaging Preference Optimization Alignment in Large Language Model for Continual Radiology Report Generation: A Hybrid Approach},
  journal      = {Cognitive Computation},
  year         = {2025},
  volume       = {17},
  number       = {1},
  pages        = {53},
  doi          = {10.1007/s12559-025-10404-6},
  url          = {https://doi.org/10.1007/s12559-025-10404-6},
  abstract     = {Large language models (LLMs) remain relatively underutilized in medical imaging, particularly in radiology, which is essential for disease diagnosis and management. Nonetheless, radiology report generation (RRG) is a time-consuming task that can result in delays and inconsistencies. To address these challenges, we present a novel hybrid approach that integrates multi-modal radiology information and preference optimization alignment in LLM for continual RRG. Our method integrates a pre-trained small multi-modal model to analyze radiology images and generate an initial report, which is subsequently refined and aligned by an LLM using odds ratio preference optimization (ORPO) and with historical patient data and assessments to mimic radiologist-like responses, bypassing reinforcement learning from human feedback-based (RLHF) alignment. This two-stage fusion—supervised fine-tuning followed by preference optimization—ensures high accuracy while minimizing hallucinations and errors. We also propose a data field curation strategy extendable to various other RRG modality datasets, focusing on selecting relevant responses for preference alignment. We evaluate our approach on two public datasets, achieving state-of-the-art performance with average Bleu scores of 0.375 and 0.647, Meteor scores of 0.495 and 0.714, Rouge-L scores of 0.483 and 0.732, and average F1-RadGraph scores of 0.488 and 0.487, for chest X-rays and lung CT scan datasets, respectively. We further provide in-depth qualitative analyses and ablation studies to explain the workings of our model and grasp the clinical relevance for RRG. This work presents the first application of preference optimization in continual RRG, representing a significant advancement in automating clinically reliable report generation. By reducing cognitive burdens on radiologists through AI-powered reasoning and alignment in LLMs, the proposed model improves decision-making, perception, and diagnostic precision, streamlining workflows and enhancing patient care. Our code is available at https://github.com/AI-14/r2gpoallm.},
  issn         = {1866-9964}
}


@INPROCEEDINGS{9706902,
  author={Wu, Lirong and Liu, Zicheng and Xia, Jun and Zang, Zelin and Li, Siyuan and Li, Stan Z.},
  booktitle={2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Generalized Clustering and Multi-Manifold Learning with Geometric Structure Preservation}, 
  year={2022},
  volume={},
  number={},
  pages={1668-1676},
  keywords={Manifolds;Measurement;Computer vision;Codes;Clustering algorithms;Biology;Deep Learning Clustering},
  doi={10.1109/WACV51458.2022.00173}}