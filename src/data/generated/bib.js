const generatedBibEntries = {
    " refId0": {
        "author": "{Lin, Run} and {Li, Chunxiao} and {Wang, Ruixuan}",
        "doi": "\"10.1051/bioconf/202517403010\",",
        "journal": "BIO Web Conf.",
        "pages": "\"03010\",",
        "title": "A Benchmark for Multi-Task Evaluation of Pretrained Models in Medical Report Generation",
        "type": "article",
        "url": "\"https://doi.org/10.1051/bioconf/202517403010\",",
        "volume": "174,",
        "year": "2025,"
    },
    "8363564": {
        "author": "Chuquicusma, Maria J. M. and Hussein, Sarfaraz and Burt, Jeremy and Bagci, Ulas",
        "booktitle": "2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)",
        "doi": "10.1109/ISBI.2018.8363564",
        "keywords": "Cancer;Lung;Visualization;Computed tomography;Generators;Gallium nitride;Lung nodules;Generated samples;Visual Turing Test;Computed Tomography (CT);Deep learning;Generative Adversarial Networks (GANs);Computer Aided Diagnosis (CAD) systems",
        "number": "",
        "pages": "240-244",
        "title": "How to fool radiologists with generative adversarial networks? A visual turing test for lung cancer diagnosis",
        "type": "INPROCEEDINGS",
        "volume": "",
        "year": "2018"
    },
    "8363576": {
        "author": "Frid-Adar, Maayan and Klang, Eyal and Amitai, Michal and Goldberger, Jacob and Greenspan, Hayit",
        "booktitle": "2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)",
        "doi": "10.1109/ISBI.2018.8363576",
        "keywords": "Lesions;Liver;Gallium nitride;Training;Medical diagnostic imaging;Task analysis;Image synthesis;data augmentation;generative adversarial network;liver lesions;lesion classification",
        "number": "",
        "pages": "289-293",
        "title": "Synthetic data augmentation using GAN for improved liver lesion classification",
        "type": "INPROCEEDINGS",
        "volume": "",
        "year": "2018"
    },
    "9706902": {
        "author": "Wu, Lirong and Liu, Zicheng and Xia, Jun and Zang, Zelin and Li, Siyuan and Li, Stan Z.",
        "booktitle": "2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",
        "doi": "10.1109/WACV51458.2022.00173",
        "keywords": "Manifolds;Measurement;Computer vision;Codes;Clustering algorithms;Biology;Deep Learning Clustering",
        "number": "",
        "pages": "1668-1676",
        "title": "Generalized Clustering and Multi-Manifold Learning with Geometric Structure Preservation",
        "type": "INPROCEEDINGS",
        "volume": "",
        "year": "2022"
    },
    "Izhar2025": {
        "abstract": "Large language models (LLMs) remain relatively underutilized in medical imaging, particularly in radiology, which is essential for disease diagnosis and management. Nonetheless, radiology report generation (RRG) is a time-consuming task that can result in delays and inconsistencies. To address these challenges, we present a novel hybrid approach that integrates multi-modal radiology information and preference optimization alignment in LLM for continual RRG. Our method integrates a pre-trained small multi-modal model to analyze radiology images and generate an initial report, which is subsequently refined and aligned by an LLM using odds ratio preference optimization (ORPO) and with historical patient data and assessments to mimic radiologist-like responses, bypassing reinforcement learning from human feedback-based (RLHF) alignment. This two-stage fusion\u2014supervised fine-tuning followed by preference optimization\u2014ensures high accuracy while minimizing hallucinations and errors. We also propose a data field curation strategy extendable to various other RRG modality datasets, focusing on selecting relevant responses for preference alignment. We evaluate our approach on two public datasets, achieving state-of-the-art performance with average Bleu scores of 0.375 and 0.647, Meteor scores of 0.495 and 0.714, Rouge-L scores of 0.483 and 0.732, and average F1-RadGraph scores of 0.488 and 0.487, for chest X-rays and lung CT scan datasets, respectively. We further provide in-depth qualitative analyses and ablation studies to explain the workings of our model and grasp the clinical relevance for RRG. This work presents the first application of preference optimization in continual RRG, representing a significant advancement in automating clinically reliable report generation. By reducing cognitive burdens on radiologists through AI-powered reasoning and alignment in LLMs, the proposed model improves decision-making, perception, and diagnostic precision, streamlining workflows and enhancing patient care. Our code is available at https://github.com/AI-14/r2gpoallm.",
        "author": "Amaan Izhar and Norisma Idris and Nurul Japar",
        "doi": "10.1007/s12559-025-10404-6",
        "issn": "1866-9964",
        "journal": "Cognitive Computation",
        "number": "1",
        "pages": "53",
        "title": "Engaging Preference Optimization Alignment in Large Language Model for Continual Radiology Report Generation: A Hybrid Approach",
        "type": "article",
        "url": "https://doi.org/10.1007/s12559-025-10404-6",
        "volume": "17",
        "year": "2025"
    },
    "KAZEMINIA2020101938": {
        "abstract": "Generative adversarial networks (GANs) and their extensions have carved open many exciting ways to tackle well known and challenging medical image analysis problems such as medical image de-noising, reconstruction, segmentation, data simulation, detection or classification. Furthermore, their ability to synthesize images at unprecedented levels of realism also gives hope that the chronic scarcity of labeled data in the medical field can be resolved with the help of these generative models. In this review paper, a broad overview of recent literature on GANs for medical applications is given, the shortcomings and opportunities of the proposed methods are thoroughly discussed, and potential future work is elaborated. We review the most relevant papers published until the submission date. For quick access, essential details such as the underlying method, datasets, and performance are tabulated. An interactive visualization that categorizes all papers to keep the review alive is available at http://livingreview.in.tum.de/GANs_for_Medical_Applications/.",
        "author": "Salome Kazeminia and Christoph Baur and Arjan Kuijper and Bram {van Ginneken} and Nassir Navab and Shadi Albarqouni and Anirban Mukhopadhyay",
        "doi": "https://doi.org/10.1016/j.artmed.2020.101938",
        "issn": "0933-3657",
        "journal": "Artificial Intelligence in Medicine",
        "keywords": "Generative adversarial networks, Deep learning, Medical imaging, Survey",
        "pages": "101938",
        "title": "GANs for medical image analysis",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S0933365719311510",
        "volume": "109",
        "year": "2020"
    },
    "SORTINO2023103721": {
        "abstract": "Graph-structured scene descriptions can be efficiently used in generative models to control the composition of the generated image. Previous approaches are based on the combination of graph convolutional networks and adversarial methods for layout prediction and image generation, respectively. In this work, we show how employing multi-head attention to encode the graph information, as well as using a transformer-based model in the latent space for image generation can improve the quality of the sampled data, without the need to employ adversarial models with the subsequent advantage in terms of training stability. The proposed approach, specifically, is entirely based on transformer architectures both for encoding scene graphs into intermediate object layouts and for decoding these layouts into images, passing through a lower dimensional space learned by a vector-quantized variational autoencoder. Our approach shows an improved image quality with respect to state-of-the-art methods as well as a higher degree of diversity among multiple generations from the same scene graph. We evaluate our approach on three public datasets: Visual Genome, COCO, and CLEVR. We achieve an Inception Score of 13.7 and 12.8, and an FID of 52.3 and 60.3, on COCO and Visual Genome, respectively. We perform ablation studies on our contributions to assess the impact of each component. Code is available at https://github.com/perceivelab/trf-sg2im.",
        "author": "Renato Sortino and Simone Palazzo and Francesco Rundo and Concetto Spampinato",
        "doi": "https://doi.org/10.1016/j.cviu.2023.103721",
        "issn": "1077-3142",
        "journal": "Computer Vision and Image Understanding",
        "keywords": "Scene graphs, Transformers, Generative models, Conditional image generation",
        "pages": "103721",
        "title": "Transformer-based image generation from scene graphs",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S1077314223001017",
        "volume": "233",
        "year": "2023"
    },
    "UsmanAkbar2024": {
        "abstract": "Large annotated datasets are required for training deep learning models, but in medical imaging data sharing is often complicated due to ethics, anonymization and data protection legislation. Generative AI models, such as generative adversarial networks (GANs) and diffusion models, can today produce very realistic synthetic images, and can potentially facilitate data sharing. However, in order to share synthetic medical images it must first be demonstrated that they can be used for training different networks with acceptable performance. Here, we therefore comprehensively evaluate four GANs (progressive GAN, StyleGAN 1\u20133) and a diffusion model for the task of brain tumor segmentation (using two segmentation networks, U-Net and a Swin transformer). Our results show that segmentation networks trained on synthetic images reach Dice scores that are 80%\u201390% of Dice scores when training with real images, but that memorization of the training images can be a problem for diffusion models if the original dataset is too small. Our conclusion is that sharing synthetic medical images is a viable option to sharing real images, but that further work is required. The trained generative models and the generated synthetic images are shared on AIDA data hub.",
        "author": "Muhammad Usman Akbar and M\u00e5ns Larsson and Ida Blystad and Anders Eklund",
        "doi": "10.1038/s41597-024-03073-x",
        "issn": "2052-4463",
        "journal": "Scientific Data",
        "number": "1",
        "pages": "259",
        "title": "Brain tumor segmentation using synthetic MR images - A comparison of GANs and diffusion models",
        "type": "article",
        "url": "https://doi.org/10.1038/s41597-024-03073-x",
        "volume": "11",
        "year": "2024"
    },
    "YI2019101552": {
        "abstract": "Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.",
        "author": "Xin Yi and Ekta Walia and Paul Babyn",
        "doi": "https://doi.org/10.1016/j.media.2019.101552",
        "issn": "1361-8415",
        "journal": "Medical Image Analysis",
        "keywords": "Deep learning, Generative adversarial network, Generative model, Medical imaging, Review",
        "pages": "101552",
        "title": "Generative adversarial network in medical imaging: A review",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S1361841518308430",
        "volume": "58",
        "year": "2019"
    },
    "bioengineering10111258": {
        "abstract": "Conditional image generation plays a vital role in medical image analysis as it is effective in tasks such as super-resolution, denoising, and inpainting, among others. Diffusion models have been shown to perform at a state-of-the-art level in natural image generation, but they have not been thoroughly studied in medical image generation with specific conditions. Moreover, current medical image generation models have their own problems, limiting their usage in various medical image generation tasks. In this paper, we introduce the use of conditional Denoising Diffusion Probabilistic Models (cDDPMs) for medical image generation, which achieve state-of-the-art performance on several medical image generation tasks.",
        "article-number": "1258",
        "author": "Hung, Alex Ling Yu and Zhao, Kai and Zheng, Haoxin and Yan, Ran and Raman, Steven S. and Terzopoulos, Demetri and Sung, Kyunghyun",
        "doi": "10.3390/bioengineering10111258",
        "issn": "2306-5354",
        "journal": "Bioengineering",
        "number": "11",
        "pubmedid": "38002382",
        "title": "Med-cDiff: Conditional Medical Image Generation with Diffusion Models",
        "type": "Article",
        "url": "https://www.mdpi.com/2306-5354/10/11/1258",
        "volume": "10",
        "year": "2023"
    }
};